# Service d'Analyse AI - Installation et Configuration

## üìã Pr√©requis

- Python 3.8+
- Node.js 14+
- npm ou yarn

## üöÄ Installation

### 1. Installation des d√©pendances Python

```bash
cd Backend
pip install -r requirements.txt
```

Si vous rencontrez des probl√®mes avec torch, installez-le s√©par√©ment :

```bash
# Pour CPU uniquement
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

# Pour CUDA (GPU NVIDIA)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### 2. Installation des d√©pendances Node.js

```bash
cd Backend
npm install form-data
```

### 3. T√©l√©charger le mod√®le YOLO

Le mod√®le YOLOv8 sera t√©l√©charg√© automatiquement au premier lancement. Pour le t√©l√©charger manuellement :

```bash
# Dans le dossier Backend
mkdir -p models
cd models
wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt
```

## üéØ D√©marrage des Services

### Option 1: D√©marrage Manuel

#### Terminal 1 - Service AI (Python)
```bash
cd Backend/src/services
python ai_analysis_server.py
```

Le service AI d√©marre sur le port **5001** par d√©faut.

#### Terminal 2 - Backend API (Node.js)
```bash
cd Backend
npm start
# ou
node src/index.js
```

Le backend API d√©marre sur le port **5000** par d√©faut.

### Option 2: D√©marrage avec Script PowerShell

Cr√©ez un fichier `start-services.ps1` :

```powershell
# D√©marrer le service AI
Start-Process powershell -ArgumentList "-NoExit", "-Command", "cd Backend/src/services; python ai_analysis_server.py"

# Attendre 3 secondes
Start-Sleep -Seconds 3

# D√©marrer le backend API
Start-Process powershell -ArgumentList "-NoExit", "-Command", "cd Backend; npm start"
```

Ex√©cutez :
```bash
.\start-services.ps1
```

### Option 3: D√©marrage avec Docker (Recommand√© pour production)

Cr√©ez un `docker-compose.yml` :

```yaml
version: '3.8'

services:
  ai-service:
    build:
      context: ./Backend
      dockerfile: Dockerfile.ai
    ports:
      - "5001:5001"
    environment:
      - AI_SERVICE_PORT=5001
      - DEBUG=False
    volumes:
      - ./AI:/app/AI
      - ./Backend/models:/app/models

  backend-api:
    build:
      context: ./Backend
      dockerfile: Dockerfile
    ports:
      - "5000:5000"
    environment:
      - AI_SERVICE_URL=http://ai-service:5001
      - MONGODB_URI=mongodb://mongo:27017/fruitytrack
    depends_on:
      - ai-service
      - mongo

  mongo:
    image: mongo:latest
    ports:
      - "27017:27017"
    volumes:
      - mongo-data:/data/db

volumes:
  mongo-data:
```

D√©marrez :
```bash
docker-compose up -d
```

## üîß Configuration

### Variables d'environnement

Cr√©ez un fichier `.env` dans le dossier Backend :

```env
# Service AI
AI_SERVICE_URL=http://localhost:5001
AI_SERVICE_PORT=5001

# Backend
PORT=5000
MONGODB_URI=mongodb://localhost:27017/fruitytrack
JWT_SECRET=your_jwt_secret_key

# Mode debug
DEBUG=False
```

### Configuration du service AI

Le service peut fonctionner en deux modes :

1. **Mode Complet** : Avec YOLO et MaskRCNN pour une analyse avanc√©e
2. **Mode Basique** : Analyse par couleur uniquement (si les d√©pendances AI ne sont pas disponibles)

Le mode est d√©tect√© automatiquement au d√©marrage.

## üì° API Endpoints

### Service AI (Port 5001)

#### Health Check
```http
GET http://localhost:5001/health
```

#### Analyser une image
```http
POST http://localhost:5001/analyze
Content-Type: multipart/form-data

{
  "file": <image_file>,
  "tree_type": "Olivier",
  "gps_data": "{\"latitude\": 36.8065, \"longitude\": 10.1815}"
}
```

#### Analyse en lot
```http
POST http://localhost:5001/batch-analyze
Content-Type: multipart/form-data

{
  "files": [<image1>, <image2>, ...]
}
```

### Backend API (Port 5000)

#### Cr√©er une analyse avec AI
```http
POST http://localhost:5000/api/analysis/create-with-ai
Authorization: Bearer <token>
Content-Type: multipart/form-data

{
  "image": <image_file>,
  "treeType": "Olivier",
  "gpsData": {
    "latitude": 36.8065,
    "longitude": 10.1815,
    "accuracy": 5.2
  },
  "notes": "Analyse de routine"
}
```

## üß™ Tests

### Tester le service AI

```bash
curl http://localhost:5001/health
```

R√©sultat attendu :
```json
{
  "status": "healthy",
  "service": "AI Analysis Service",
  "version": "1.0.0"
}
```

### Tester l'analyse avec une image

```bash
curl -X POST http://localhost:5001/analyze \
  -F "file=@test_tree.jpg" \
  -F "tree_type=Olivier"
```

### Test depuis l'application mobile

1. Ouvrez l'app mobile
2. Allez dans "Analyse d'Arbre"
3. Prenez une photo
4. S√©lectionnez le type d'arbre
5. Lancez l'analyse

Les logs devraient montrer :
```
ü§ñ Lancement de l'analyse AI...
‚úÖ Service AI complet charg√©
üìä Analyse cr√©√©e avec succ√®s
```

## üìä Monitoring et Logs

### Logs du service AI

Les logs sont affich√©s dans le terminal du service AI :

```
INFO: üöÄ D√©marrage du service AI sur le port 5001
INFO: ‚úÖ Mod√®le YOLO charg√©
INFO: ‚úÖ Mod√®le MaskRCNN charg√© sur cpu
INFO: Analyse de l'image: /tmp/tree_analysis_image.jpg
```

### Logs du backend

Les logs sont affich√©s dans le terminal du backend :

```
ü§ñ Lancement de l'analyse AI...
‚úÖ Arbre trouv√© √† 5.23m
üìä Analyse cr√©√©e avec succ√®s
```

## ‚ö†Ô∏è D√©pannage

### Le service AI ne d√©marre pas

1. V√©rifier que Python 3.8+ est install√© :
   ```bash
   python --version
   ```

2. V√©rifier les d√©pendances :
   ```bash
   pip list | grep -E "Flask|opencv|torch|ultralytics"
   ```

3. V√©rifier les ports :
   ```bash
   netstat -an | findstr "5001"
   ```

### Erreur "Service AI non disponible"

1. V√©rifier que le service AI est d√©marr√©
2. V√©rifier l'URL dans `.env` :
   ```env
   AI_SERVICE_URL=http://localhost:5001
   ```
3. Tester la connexion :
   ```bash
   curl http://localhost:5001/health
   ```

### Performances lentes

1. **Utiliser un GPU** : Si disponible, installez torch avec CUDA
2. **R√©duire la taille des images** : Redimensionner avant l'upload
3. **Ajuster le timeout** : Dans `aiService.js`, augmentez `this.timeout`

### Erreur de m√©moire

1. R√©duire le nombre d'analyses simultan√©es
2. Utiliser le mode basique (sans AI avanc√©e)
3. Augmenter la m√©moire allou√©e :
   ```bash
   export NODE_OPTIONS="--max-old-space-size=4096"
   ```

## üöÄ Optimisations Production

### 1. Utiliser un serveur WSGI

```bash
pip install gunicorn

# D√©marrer avec gunicorn
gunicorn -w 4 -b 0.0.0.0:5001 ai_analysis_server:app
```

### 2. Activer le cache

Modifier `ai_analysis_server.py` pour ajouter un cache :

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def analyze_cached(image_hash):
    # Analyse avec cache
    pass
```

### 3. Load Balancing

Utiliser nginx pour distribuer la charge :

```nginx
upstream ai_service {
    server 127.0.0.1:5001;
    server 127.0.0.1:5002;
    server 127.0.0.1:5003;
}

server {
    location /analyze {
        proxy_pass http://ai_service;
    }
}
```

### 4. Monitoring avec PM2

```bash
npm install -g pm2

# D√©marrer les services
pm2 start ai_analysis_server.py --name ai-service --interpreter python
pm2 start src/index.js --name backend-api

# Voir les logs
pm2 logs

# Monitoring
pm2 monit
```

## üìà M√©triques et Performance

### Temps d'analyse moyens

- **Mode Basique** : 0.5-1 seconde
- **Mode YOLO** : 2-5 secondes
- **Mode Complet (YOLO + MaskRCNN)** : 5-10 secondes

### Utilisation M√©moire

- **Service AI (basique)** : ~200MB
- **Service AI (YOLO)** : ~1.5GB
- **Service AI (complet)** : ~3GB

### Pr√©cision

- **D√©tection de maladies** : 75-90%
- **Analyse de sant√©** : 80-95%
- **Segmentation d'arbre** : 85-95%

## üìö Ressources

- [Documentation YOLO](https://docs.ultralytics.com/)
- [PyTorch](https://pytorch.org/docs/stable/index.html)
- [Flask](https://flask.palletsprojects.com/)
- [OpenCV](https://docs.opencv.org/)

## üÜò Support

Pour toute question ou probl√®me :
1. V√©rifier les logs
2. Consulter cette documentation
3. V√©rifier les issues GitHub
4. Contacter l'√©quipe de d√©veloppement
